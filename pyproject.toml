[project]
name = "bada"
version = "0.1.0"
description = "Business-Aware agentic gui framework"
readme = "README.md"
requires-python = "==3.10.*"
dependencies = [
    "accelerate==1.2.1",
    "auto-gptq==0.7.1",
    "jax==0.4.38",
    "langchain==0.3.12",
    "langchain-community==0.3.12",
    "llama-cpp-python==0.3.*",
    "optimum==1.23.3",
    "qwen-vl-utils==0.0.8",
    "torch==2.5.1",
    "torchvision==0.20.1",
    "transformers==4.47.1",
]

[tool.black]
line-length = 120

[tool.flake8]
max-line-length = 120
ignore = ["F841","E203","E501"]

[tool.mypy]
ignore_missing_imports = true
explicit_package_bases = true

[tool.uv.sources]
torch = [
  { index = "torch-cpu", marker = "platform_system == 'darwin'"},
  { index = "torch-gpu", marker = "platform_system != 'darwin'"},
]
torchvision = [
  { index = "torch-cpu", marker = "platform_system == 'darwin'"},
  { index = "torch-gpu", marker = "platform_system != 'darwin'"},
]
llama-cpp-python = [
  { index = "llama-cpp-python-cpu", marker = "platform_system == 'darwin'"},
  { index = "llama-cpp-python-gpu", marker = "platform_system != 'darwin'"},
]

[[tool.uv.index]]
name = "torch-cpu"
url = "https://download.pytorch.org/whl/cpu"

[[tool.uv.index]]
name = "torch-gpu"
url = "https://download.pytorch.org/whl/cu124"

[[tool.uv.index]]
name = "llama-cpp-python-cpu"
url = "https://abetlen.github.io/llama-cpp-python/whl/cpu"

[[tool.uv.index]]
name = "llama-cpp-python-gpu"
url = "https://abetlen.github.io/llama-cpp-python/whl/cu124"

[dependency-groups]
dev = [
    "pre-commit==4.0.1",
]
